{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aef4d90e",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217d0bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from bs4 import BeautifulSoup\n",
    "# import urllib2\n",
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "import sys\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1dd088",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('SentiWordNet_3.0.0_20130122.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685b83df",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = file.readlines()\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cfa87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines[len(lines)-2].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0e070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sent = {}\n",
    "for i in range(27,len(lines)-1):\n",
    "    curr_list = lines[i].split()\n",
    "    word=curr_list[4].rsplit('#', 1)[0]\n",
    "    word_sent.setdefault(word, []).append((float(curr_list[2]), float(curr_list[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3db5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines[4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d7db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6cf9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "\n",
    "for key in word_sent:\n",
    "#     print(key)\n",
    "    if len(word_sent[key])>1:#print key\n",
    "        word_sent[key]=tuple(map(mean, zip(*word_sent[key])))\n",
    "    else:\n",
    "        word_sent[key]=word_sent[key][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9456a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d545c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=pd.read_csv(\"english_tweets.csv\")\n",
    "tweets_translated=pd.read_csv(\"translatedTweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0455e17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b44993",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_translated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5b64cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tweets_translated))\n",
    "print(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d782829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('all')\n",
    "\n",
    "from pattern import en\n",
    "from pattern.en import parse\n",
    "from pattern.en import pprint\n",
    "from pattern.vector import stem, PORTER, LEMMA\n",
    "\n",
    "punctuation = list('.,;:!?()[]{}`''\\\"@#$^&*+-|=~_')\n",
    "from sklearn.feature_extraction import text \n",
    "stopwords=text.ENGLISH_STOP_WORDS\n",
    "import re\n",
    "regex1=re.compile(r\"\\.{2,}\")\n",
    "regex2=re.compile(r\"\\-{2,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bc14ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this converts tweet to nouns and adjectives in their basic forms\n",
    "def get_parts(thetext):\n",
    "    thetext=re.sub(regex1, ' ', thetext)\n",
    "    thetext=re.sub(regex2, ' ', thetext)\n",
    "    nouns_or_adj=[]\n",
    "    print(thetext.split())\n",
    "    for sentence in parse(thetext, tokenize=True, lemmata=True).split():\n",
    "        for token in sentence:\n",
    "            #print token\n",
    "            if len(token[4]) >0:\n",
    "                if token[1] in ['JJ', 'JJR', 'JJS','NN','NNS']:\n",
    "                    if token[4] in stopwords or token[4][0] in punctuation or token[4][-1] in punctuation or len(token[4])==1:\n",
    "                        continue\n",
    "                    nouns_or_adj.append(token[4].lower()) #make all lowercase\n",
    "    return nouns_or_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d480c7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiGraderNew(tweet):\n",
    "    pos=[]\n",
    "    neg=[]\n",
    "    for word in get_parts(row.Text):\n",
    "        if word in word_sent.keys():\n",
    "            pos.append(word_sent[word][0])\n",
    "            neg.append(word_sent[word][1])\n",
    "    return pos,neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74431b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampletweettext=tweets.iloc[3].Text\n",
    "sampletweettext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c60f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(get_parts(sampletweettext)) \n",
    "except:\n",
    "    StopIteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6587b4",
   "metadata": {},
   "source": [
    "# English Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939235ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mean_pos=[]\n",
    "mean_neg=[]\n",
    "total_pos=[]\n",
    "total_neg=[]\n",
    "for i in range(len(tweets)):\n",
    "    row=tweets.iloc[i]\n",
    "    pos=sentiGraderNew(row.Text)[0]\n",
    "    neg=sentiGraderNew(row.Text)[1]\n",
    "    mean_pos.append(np.mean(pos) if len(pos)>0 else 0)\n",
    "    mean_neg.append(np.mean(neg) if len(neg)>0 else 0)\n",
    "    total_pos.append(np.sum(x>0.5 for x in pos))\n",
    "    total_neg.append(np.sum(x>0.5 for x in neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9827cc3",
   "metadata": {},
   "source": [
    "# Non-English Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00fb8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#translated tweets\n",
    "mean_pos_trans=[]\n",
    "mean_neg_trans=[]\n",
    "total_pos_trans=[]\n",
    "total_neg_trans=[]\n",
    "for i in range(len(tweets_translated)):\n",
    "    row=tweets_translated.iloc[i]\n",
    "    pos=sentiGraderNew(row.Text)[0]\n",
    "    neg=sentiGraderNew(row.Text)[1]\n",
    "    mean_pos_trans.append(np.mean(pos) if len(pos)>0 else 0)\n",
    "    mean_neg_trans.append(np.mean(neg) if len(neg)>0 else 0)\n",
    "    total_pos_trans.append(np.sum(x>0.5 for x in pos))\n",
    "    total_neg_trans.append(np.sum(x>0.5 for x in neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d9dca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tweets_with_sent=tweets\n",
    "eng_tweets_with_sent['mean_pos']=mean_pos\n",
    "eng_tweets_with_sent['mean_neg']=mean_neg\n",
    "eng_tweets_with_sent['total_pos']=total_pos\n",
    "eng_tweets_with_sent['total_neg']=total_neg\n",
    "eng_tweets_with_sent.to_csv(\"eng_tweets_with_sent.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e63247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trans_tweets_with_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1026bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames=[eng_tweets_with_sent]\n",
    "all_tweets_with_sent=pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3915d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_features=all_tweets_with_sent.groupby(['Company',\"Series\"]).mean()\n",
    "sentiment_features.to_csv(\"sentiment_features\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d06a093",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9574dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac61d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
